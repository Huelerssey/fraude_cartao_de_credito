import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt


# fun√ß√£o que otimiza o carregamento dos dados
@st.cache_data
def carregar_dados():
    tabela = pd.read_pickle('arquivos_pkl/dataframe_fraude.pkl')
    return tabela

def storytelling():
    st.markdown("<h1 style='text-align: center;'>üìå Storytelling do Projeto üìå</h1>", unsafe_allow_html=True)
    st.write("")

    st.header("üìå Introdu√ß√£o")
    st.write("Bem-vindo ao projeto de detec√ß√£o de fraudes em cart√µes de cr√©dito! Aqui, voc√™ vai me acompanhar em uma jornada completa, explorando todos os aspectos que envolvem desde a obten√ß√£o dos dados at√© o compartilhamento dos resultados. Ao longo deste storytelling, vamos executar as etapas de extra√ß√£o, limpeza e transforma√ß√£o dos dados (ELT), mergulhar em t√©cnicas de modelagem estat√≠stica e machine learning, e por fim, apresentar os insights e descobertas que nos ajudar√£o a combater efetivamente as fraudes nesse cen√°rio.")
    st.image("imagens/1.png")
    st.write("")

    st.header("üìå Obten√ß√£o dos dados")
    st.write("Na fase de obten√ß√£o dos dados, recorri √† plataforma Kaggle, uma fonte confi√°vel de conjuntos de dados reais para cientistas de dados. Para este projeto, utilizei dados fornecidos por uma institui√ß√£o financeira europeia, dispon√≠veis neste [link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). Esses dados representam transa√ß√µes de cart√µes de cr√©dito e s√£o essenciais para o desenvolvimento da an√°lise e detec√ß√£o de fraudes.")
    st.image("imagens/2.png")
    st.write("")

    st.header("üìå Entendimento da √°rea/neg√≥cio")
    st.write("Vamos come√ßar entendendo a base de dados, aqui est√° uma tabela com as 5 primeiras linhas do nosso dataset.")
    st.dataframe(carregar_dados().head(5), hide_index=True)
    st.write("‚Ä¢ Coluna 'Time' (tempo): cont√©m os segundos decorridos entre cada transa√ß√£o e a primeira transa√ß√£o.")
    st.write("‚Ä¢ Coluna 'Amount' (valor): cont√©m o valor da transa√ß√£o.")
    st.write("‚Ä¢ Colunas 'V1, V2...V28' (caracteristicas): s√£o as caracteristicas da transa√ß√£o que passaram por um processo de PCA, ou seja, os dados foram transformados de maneira que n√£o seja poss√≠vel indetificar o dono real da transa√ß√£o, para garantir a confidencialidade.")
    st.write("‚Ä¢ Coluna 'Class' (classe): √© a vari√°vel de resposta onde toda vez que o valor for 1, a transa√ß√£o √© uma fraude e quando for 0, significa que a transa√ß√£o n√£o √© fraudolenta.")
    st.write("Ent√£o, atrav√©s desses dados (Tempo, Valor e Caracteristicas), vamos desenvolver um modelo que seja capaz de prever se uma transa√ß√£o √© ou n√£o fraudolenta.")
    st.write("")

    st.header("üìå Limpeza e tratamento dos dados")
    st.write("Come√ßando com a visualisa√ß√£o estat√≠stica da base de dados:")
    codigo1 = """
    print(tabela.info())
    RangeIndex: 284807 entries, 0 to 284806
    Data columns (total 31 columns):
    #   Column  Non-Null Count   Dtype
    ---  ------  --------------   -----
    0   Time    284807 non-null  float64
    1   V1      284807 non-null  float64
    2   V2      284807 non-null  float64
    3   V3      284807 non-null  float64
    4   V4      284807 non-null  float64
    5   V5      284807 non-null  float64
    6   V6      284807 non-null  float64
    7   V7      284807 non-null  float64
    8   V8      284807 non-null  float64
    9   V9      284807 non-null  float64
    10  V10     284807 non-null  float64
    11  V11     284807 non-null  float64
    12  V12     284807 non-null  float64
    13  V13     284807 non-null  float64
    14  V14     284807 non-null  float64
    15  V15     284807 non-null  float64
    16  V16     284807 non-null  float64
    17  V17     284807 non-null  float64
    18  V18     284807 non-null  float64
    19  V19     284807 non-null  float64
    20  V20     284807 non-null  float64
    21  V21     284807 non-null  float64
    22  V22     284807 non-null  float64
    23  V23     284807 non-null  float64
    24  V24     284807 non-null  float64
    25  V25     284807 non-null  float64
    26  V26     284807 non-null  float64
    27  V27     284807 non-null  float64
    28  V28     284807 non-null  float64
    29  Amount  284807 non-null  float64
    30  Class   284807 non-null  int64
    dtypes: float64(30), int64(1)
    """
    st.code(codigo1, language='python')
    st.write("Ao analisar o c√≥digo, √© poss√≠vel constatar que a base de dados inicial est√° uniforme, o que √© algo relativamente raro e extremamente positivo. Essa uniformidade nos dados economiza um tempo valioso durante as etapas de pr√©-processamento e limpeza, permitindo um foco maior na an√°lise e detec√ß√£o de padr√µes e fraudes.")
    st.write("")

    st.header("üìå An√°lise explorat√≥ria de dados")
    st.write("Nesta etapa, aplicarei meus conhecimentos em estat√≠stica e programa√ß√£o para preparar os dados e torn√°-los prontos para serem utilizados em um modelo de previs√£o. Como este √© um projeto de portf√≥lio, vou compartilhar a parte mais visual e explicativa do processo. Caso tenha interesse, voc√™ pode acessar o c√≥digo-fonte completo da aplica√ß√£o no meu GitHub, dispon√≠vel neste [link](https://github.com/Huelerssey/fraude_cartao_de_credito).")
    st.write("Vamos iniciar exibindo um gr√°fico que apresenta a porcentagem de transa√ß√µes fraudulentas em nossa base de dados:")
    st.image("imagens/3.png")
    st.write("√â evidente que h√° uma porcentagem m√≠nima de fraudes, o que impacta diretamente na maneira como avaliaremos nosso modelo de machine learning e nada melhor do que um exemplo pr√°tico para demonstrar isso.")
    st.write("O trecho de c√≥digo abaixo √© respons√°vel por treinar e testar um modelo de intelig√™ncia artificial utilizando a acur√°cia como m√©todo de avalia√ß√£o.")
    codigo2 = """
    # definindo dados de treino e de teste
    y = tabela['Class']
    x = tabela.drop('Class', axis=1)

    # dividindo a base entre treino e teste
    x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.30, random_state=42, stratify=y)

    # criando uma IA
    clf = tree.DecisionTreeClassifier(random_state=42)

    # treina a IA
    clf = clf.fit(x_treino, y_treino)

    # previs√£o da IA
    y_pred = clf.predict(x_teste)

    # Avaliando a IA
    accuracy = accuracy_score(y_teste, y_pred)
    accuracy_percent = accuracy * 100
    accuracy_formatted = f"{accuracy_percent:.2f}%"

    # Cria um cart√£o (figura) vazio
    fig, ax = plt.subplots(figsize=(4, 3))

    # Adiciona o texto do accuracy_formatted no cart√£o
    ax.text(0.5, 0.5, accuracy_formatted, ha='center', va='center', fontsize=24)

    # Remove os eixos e limites do gr√°fico
    ax.axis('off')

    # Exibe o resultado
    plt.show()
    """
    st.code(codigo2, language='python')
    st.image("imagens/4.png")
    st.write("O resultado obtido foi de 99,92% de acur√°cia. Isso significa que constru√≠mos um modelo de previs√£o perfeito, certo? Errado! A maioria das transa√ß√µes n√£o s√£o fraudulentas, ent√£o se o modelo simplesmente considerar que todas as novas transa√ß√µes n√£o s√£o fraudes, ele ter√° uma alta taxa de acertos. Percebe como isso afeta nosso resultado? Afinal, cada fraude n√£o prevista representa um impacto financeiro negativo para a empresa.")
    st.write("E como podemos resolver esse problema? Existem duas solu√ß√µes baseadas no mesmo princ√≠pio: tornar os dados proporcionais, ou seja, igualar o n√∫mero de fraudes e n√£o fraudes. No entanto, essa √© uma tarefa complexa e delicada. Vou te ajudar a visualizar esses cen√°rios a seguir:")
    st.write("Se optarmos por excluir a maioria das transa√ß√µes n√£o fraudulentas, corremos o risco de ter os dados restantes agrupados de uma maneira que afete nossa an√°lise. Por exemplo: se excluirmos aleatoriamente a maioria dos dados e restarem apenas transa√ß√µes de alto valor. Voc√™ concorda que nem toda fraude est√° associada a uma compra de alto valor? Nesse caso, se o modelo tentar prever uma nova compra com valor mediano, a probabilidade de n√£o consider√°-la como fraude ser√° alta, resultando em preju√≠zos desnecess√°rios.")
    st.write("Por outro lado, se apenas adicionarmos mais fraudes, corremos o risco de duplicar tanto um determinado dado que isso pode prejudicar a tomada de decis√£o do modelo. Por exemplo: suponha que exista uma compra de 50 EUR registrada em nossa base de dados atual e que tenha sido classificada como fraude. Ao adotar o m√©todo de aumentar a quantidade de fraudes para equilibrar a propor√ß√£o, essa transa√ß√£o foi duplicada 200 mil vezes. A chance de o modelo de previs√£o considerar todas as novas transa√ß√µes de 50 EUR como fraudes ser√° extremamente alta, o que est√° matematicamente correto, mas n√£o reflete nossa realidade.")
    st.write("A ideia geral da solu√ß√£o est√° representada no gr√°fico abaixo, e ser√° cuidadosamente implementada com base em conhecimentos s√≥lidos de estat√≠stica e programa√ß√£o. Nosso objetivo √© desenvolver um modelo capaz de identificar fraudes com alta precis√£o, levando em considera√ß√£o as caracter√≠sticas e particularidades deste contexto banc√°rio.")
    st.image("imagens/5.png")
    st.write("")

    st.header("üìå Modelando uma intelig√™ncia artificial")
    st.write("Chegamos √† parte mais empolgante do projeto, onde constru√≠mos a intelig√™ncia artificial respons√°vel por detectar fraudes em cart√µes de cr√©dito. Embora seja uma etapa t√©cnica e complexa, vou explicar de forma simplificada como tudo funciona e compartilhar os resultados obtidos. Novamente, convido voc√™ a acessar meu GitHub, por meio deste [link](https://github.com/Huelerssey/fraude_cartao_de_credito), caso queira entender detalhadamente como essas intelig√™ncias artificiais foram modeladas, linha por linha de c√≥digo.")
    st.write("Ao lidar com a problem√°tica das fraudes, temos um desafio de classifica√ß√£o em m√£os. Para enfrent√°-lo, utilizamos tr√™s principais intelig√™ncias artificiais: Decision Tree, Random Forest e Extra Trees. Em seguida, modelamos a base de dados para contornar os problemas j√° explicados durante a an√°lise explorat√≥ria, garantindo que cada intelig√™ncia artificial utilize essa base de dados ajustada. Por fim, avaliamos o desempenho de cada abordagem e de cada uma das intelig√™ncias artificiais. Use a seguinte legenda:")
    st.write("Intelig√™ncia artificial - M√©todo de reajustar base de dados")
    
    # Lista de modelos e m√©todos de resampling
    lista_resultados = [
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Random Undersample',
            'matriz_confusao': [[75628, 9667], [16, 132]],
            'recall': 0.89
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Undersample ClusterCentroid',
            'matriz_confusao': [[42256, 43039], [7, 141]],
            'recall': 0.95
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Undersample NearMiss',
            'matriz_confusao': [[33992, 51303], [7, 141]],
            'recall': 0.95
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Random Oversample',
            'matriz_confusao': [[84787, 508], [25, 123]],
            'recall': 0.83
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Oversample SMOTE',
            'matriz_confusao': [[85142, 153], [39, 109]],
            'recall': 0.74
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Oversample ADASYN',
            'matriz_confusao': [[85151, 144], [42, 106]],
            'recall': 0.72
        },
        {
            'modelo': 'Decision Tree',
            'metodo_resampling': 'Combined Over/Undersample',
            'matriz_confusao': [[85109, 186], [36, 112]],
            'recall': 0.76
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Random Undersample',
            'matriz_confusao': [[83647, 1648], [18, 130]],
            'recall': 0.88
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Undersample ClusterCentroid',
            'matriz_confusao': [[34241, 51054], [2, 146]],
            'recall': 0.99
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Undersample NearMiss',
            'matriz_confusao': [[69222, 16073], [8, 140]],
            'recall': 0.95
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Random Oversample',
            'matriz_confusao': [[85131, 164], [26, 122]],
            'recall': 0.82
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Oversample SMOTE',
            'matriz_confusao': [[85278, 17], [30, 118]],
            'recall': 0.80
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Oversample ADASYN',
            'matriz_confusao': [[85274, 21], [29, 119]],
            'recall': 0.80
        },
        {
            'modelo': 'Random Forest',
            'metodo_resampling': 'Combined Over/Undersample',
            'matriz_confusao': [[85268, 27], [27, 121]],
            'recall': 0.82
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Random Undersample',
            'matriz_confusao': [[84320, 975], [22, 126]],
            'recall': 0.85
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Undersample ClusterCentroid',
            'matriz_confusao': [[38421, 46874], [0, 148]],
            'recall': 1.00
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Undersample NearMiss',
            'matriz_confusao': [[78545, 6750], [10, 138]],
            'recall': 0.93
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Random Oversample',
            'matriz_confusao': [[85206, 89], [26, 122]],
            'recall': 0.82
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Oversample SMOTE',
            'matriz_confusao': [[85281, 14], [29, 119]],
            'recall': 0.80
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Oversample ADASYN',
            'matriz_confusao': [[85280, 15], [29, 119]],
            'recall': 0.80
        },
        {
            'modelo': 'Extra Trees',
            'metodo_resampling': 'Combined Over/Undersample',
            'matriz_confusao': [[85278, 17], [28, 120]],
            'recall': 0.81
        },
    ]

    # Op√ß√µes dispon√≠veis no seletor
    opcoes = [f"{resultado['modelo']} - {resultado['metodo_resampling']}" for resultado in lista_resultados]

    # Seletor de op√ß√µes
    opcao_selecionada = st.selectbox("Selecione uma op√ß√£o:", opcoes)

    # Encontrar o resultado correspondente √† op√ß√£o selecionada
    resultado_selecionado = None
    for resultado in lista_resultados:
        if f"{resultado['modelo']} - {resultado['metodo_resampling']}" == opcao_selecionada:
            resultado_selecionado = resultado
            break

    # Verificar se um resultado v√°lido foi selecionado
    if resultado_selecionado is not None:
        # Dados da matriz de confus√£o
        matriz_confusao = resultado_selecionado['matriz_confusao']

        # Cores das fatias para cada gr√°fico
        cores_fatias1 = ['#00FF00', '#FF0000']
        cores_fatias2 = ['#FF0000', '#00FF00']

        # Legendas
        legenda1 = ['Errou - Modelo n√£o classificou como Fraude', 'Acertou - Modelo classificou como Fraude']
        legenda2 = ['Acertou - Modelo n√£o classificou como Fraude', 'Errou - Modelo classificou como Fraude']

        st.subheader("Tentando prever Dados que eram Fraude")
        # Gr√°fico dos dados que eram fraude
        plt.figure(figsize=(6, 6))
        plt.pie(matriz_confusao[1], colors=cores_fatias2, autopct='%1.1f%%', startangle=90)
        plt.legend(legenda1, loc='upper left', bbox_to_anchor=(0.80, 0.80), bbox_transform=plt.gcf().transFigure)
        plt.axis('equal')

        # Exibir o gr√°fico no Streamlit
        st.pyplot(plt)

        st.subheader("Tentando prever Dados que n√£o eram Fraude")
        # Gr√°fico dos dados que n√£o eram fraude
        plt.figure(figsize=(6, 6))
        plt.pie(matriz_confusao[0], colors=cores_fatias1, autopct='%1.1f%%', startangle=90)
        plt.legend(legenda2, loc='upper left', bbox_to_anchor=(0.80, 0.80), bbox_transform=plt.gcf().transFigure)
        plt.axis('equal')

        # Exibir o gr√°fico no Streamlit
        st.pyplot(plt)
    st.write("")

    st.header("üìå Resultados")
    st.write("Chegamos ao momento de apresentar os resultados obtidos deste projeto de detec√ß√£o de fraudes em cart√µes de cr√©dito. Com base na aplica√ß√£o das diferentes intelig√™ncias artificiais e na an√°lise dos dados. Vamos come√ßar respondendo a pergunta mais importante: 'Ent√£o afinal, os resultados obtidos nos gr√°ficos a cima, s√£o bons ou ruins ?' E a resposta √©: Depende!")
    st.write("Para facilitar o entendimento, vamos utilizar exemplos pr√°ticos para ilustrar os resultados obtidos. A seguir, apresentamos tr√™s modelos aleat√≥rios selecionados entre todos os utilizados para a demonstra√ß√£o.")
    st.write("")

    # modelo 1
    st.subheader("Decision Tree - Random Undersample")

    # Cria um DataFrame
    data1 = {'': ['O dado N√£o era Fraude', 'O dado Era Fraude'],
            'Modelo previu como: N√£o √© Fraude': [75628, 16],
            'Modelo previu como: √â Fraude': [9667, 132]}

    df1 = pd.DataFrame(data1)

    # Aplica a formata√ß√£o condicional para colorir c√©lulas espec√≠ficas
    def highlight_value(val):
        if val == 75628 or val == 132:  # Valores para destacar em verde
            return 'background-color: green'
        elif val == 16 or val == 9667:  # Valores para destacar em vermelho
            return 'background-color: red'
        else:
            return ''

    # Exibe a tabela com a formata√ß√£o condicional
    st.table(df1.style.applymap(highlight_value, subset=['Modelo previu como: N√£o √© Fraude', 'Modelo previu como: √â Fraude']))
    st.write("Neste modelo, observamos uma alta efic√°cia na classifica√ß√£o de transa√ß√µes fraudulentas. Das 148 totais, o modelo identificou corretamente 132 delas, o que representa uma √≥tima taxa de acerto. No entanto, qual o pre√ßo disso?")
    st.write("Ao analisar as transa√ß√µes que n√£o eram fraudulentas, o modelo classificou 9667 delas como fraude. Esses falsos positivos resultam em clientes insatisfeitos com compras negadas, gerando um alto volume de contatos com o banco e exigindo um aumento no tamanho da equipe de atendimento ao cliente e toda a infraestrutura em volta disso. Se o custo de manter uma equipe maior ultrapassar muito a economia das fraudes previstas, a utiliza√ß√£o desse modelo pode se tornar invi√°vel para a empresa.")
    st.write("")

    # modelo 2
    st.subheader("Random Forest - Oversample SMOTE")

    # Cria um DataFrame
    data2 = {'': ['O dado N√£o era Fraude', 'O dado Era Fraude'],
            'Modelo previu como: N√£o √© Fraude': [85278, 30],
            'Modelo previu como: √â Fraude': [17, 118]}

    df2 = pd.DataFrame(data2)

    # Aplica a formata√ß√£o condicional para colorir c√©lulas espec√≠ficas
    def highlight_value(val):
        if val == 85278 or val == 118:  # Valores para destacar em verde
            return 'background-color: green'
        elif val == 30 or val == 17:  # Valores para destacar em vermelho
            return 'background-color: red'
        else:
            return ''

    # Exibe a tabela com a formata√ß√£o condicional
    st.table(df2.style.applymap(highlight_value, subset=['Modelo previu como: N√£o √© Fraude', 'Modelo previu como: √â Fraude']))
    st.write("Neste outro modelo, temos  o exato oposto, com foco na redu√ß√£o dos falsos positivos. Isso significa que o modelo √© mais cauteloso na classifica√ß√£o de transa√ß√µes como fraudes, evitando assim o problema do alto volume de contatos com o banco. No entanto, essa redu√ß√£o de falsos positivos vem com o custo de uma diminui√ß√£o na precis√£o na detec√ß√£o de fraudes.")
    st.write("Observamos que, nesse modelo, o n√∫mero de transa√ß√µes fraudulentas n√£o previstas √© quase o dobro do modelo anterior. Caso uma ou mais dessas 30 fraudes n√£o detectadas sejam de alto valor, o impacto financeiro pode ser consider√°vel. √â importante ressaltar que mesmo com 16 fraudes n√£o detectadas, o risco de preju√≠zo ainda existe, mas quanto maior o n√∫mero de fraudes n√£o identificadas, maior √© a possibilidade de preju√≠zos.")
    st.write("")

    # modelo 3
    st.subheader("Extra Trees - Undersample ClusterCentroid")

    # Cria um DataFrame
    data3 = {'': ['O dado N√£o era Fraude', 'O dado Era Fraude'],
            'Modelo previu como: N√£o √© Fraude': [38421, 0],
            'Modelo previu como: √â Fraude': [46874, 148]}

    df3 = pd.DataFrame(data3)

    # Aplica a formata√ß√£o condicional para colorir c√©lulas espec√≠ficas
    def highlight_value(val):
        if val == 38421 or val == 148 or val == 0:  # Valores para destacar em verde
            return 'background-color: green'
        elif val == 46874:  # Valores para destacar em vermelho
            return 'background-color: red'
        else:
            return ''

    # Exibe a tabela com a formata√ß√£o condicional
    st.table(df3.style.applymap(highlight_value, subset=['Modelo previu como: N√£o √© Fraude', 'Modelo previu como: √â Fraude']))
    st.write("E se voc√™ est√° se perguntando se √© poss√≠vel atingir um modelo 'perfeito', aqui est√° o resultado. Essa intelig√™ncia artificial √© capaz de prever 100% das fraudes, por√©m, o custo disso √© classificar erroneamente transa√ß√µes que n√£o s√£o fraudes como fraude, 46 mil vezes.")
    st.write("")

    st.header("üìå Considera√ß√µes finais")
    st.write("Deu para perceber que √© crucial encontrar um equil√≠brio entre a quantidade de falsos positivos e falsos negativos nas previs√µes, certo? No entanto, √© importante ressaltar que, decidir se o resultado do modelo √© bom ou n√£o, s√≥ depende da √°rea de neg√≥cio ou empresa em quest√£o, pois cada uma possui suas pr√≥prias necessidades e restri√ß√µes.")
    st.write("Vale lembrar tamb√©m que n√£o √© necess√°rio usar apenas um modelo. Seria uma estrat√©gia interessante por exemplo, agrupar as transa√ß√µes ocorridas em transa√ß√µes de alto, m√©dio e pequeno porte. E utilizar um modelo espec√≠fico para cada grupo. Assim, para transa√ß√µes de grande porte utilizar modelos mais precisos e conservadores e se caso o modelo erre, exista um time respons√°vel por entrar em contato com o titular para verificar se a compra √© mesmo ver√≠dica. Enquanto nas compras de pequeno porte, usar modelos mais flex√≠veis na detec√ß√£o e em caso de falso positivo, dar a liberdado para o pr√≥prio usu√°rio resolver o problema da compra bloqueada pelo aplicativo do banco sem precisar entrar em contato.")
    st.write("Al√©m disso, √© fundamental destacar a import√¢ncia cont√≠nua da atualiza√ß√£o e monitoramento dos modelos de detec√ß√£o de fraudes, uma vez que os m√©todos utilizados pelos fraudadores est√£o em constante evolu√ß√£o.")
    st.write("Por fim, este projeto proporcionou uma experi√™ncia pr√°tica e aprofundada na √°rea de detec√ß√£o de fraudes em cart√µes de cr√©dito, consolidando os conhecimentos adquiridos em estat√≠stica, programa√ß√£o e intelig√™ncia artificial.")
    st.write("")
    st.image("imagens/6.png")

    #footer
    with st.container():
        col1, col2, col3 = st.columns(3)
        
        col2.write("Developed By: [@Huelerssey](https://github.com/Huelerssey)")